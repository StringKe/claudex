# Claudex Configuration
# Copy this file to ~/.config/claudex/config.toml and edit

# Path to claude binary (default: "claude" from PATH)
# claude_binary = "/usr/local/bin/claude"

# Proxy settings
proxy_port = 13456
proxy_host = "127.0.0.1"

# Log level: trace, debug, info, warn, error
log_level = "info"

# Terminal hyperlinks (OSC 8): "auto" | true | false
# "auto" detects terminal support; true/false force on/off
hyperlinks = "auto"

# Model aliases (shorthand → full model name)
[model_aliases]
grok3 = "grok-3-beta"
gpt4o = "gpt-4o"
ds3 = "deepseek-chat"
claude = "claude-sonnet-4-20250514"

# ─── Profiles ───────────────────────────────────────────

# Anthropic (native, no translation needed)
[[profiles]]
name = "anthropic"
provider_type = "DirectAnthropic"
base_url = "https://api.anthropic.com"
api_key = "YOUR_ANTHROPIC_KEY"   # or use api_key_keyring = "anthropic"
default_model = "claude-sonnet-4-20250514"
enabled = true
priority = 100

# MiniMax (Anthropic-compatible)
[[profiles]]
name = "minimax"
provider_type = "DirectAnthropic"
base_url = "https://api.minimax.io/anthropic"
api_key = "YOUR_API_KEY"
default_model = "claude-sonnet-4-20250514"
backup_providers = ["anthropic"]
enabled = true
priority = 90

# OpenRouter (needs translation — NOT Anthropic-compatible for passthrough)
[[profiles]]
name = "openrouter"
provider_type = "OpenAICompatible"
base_url = "https://openrouter.ai/api/v1"
api_key = "YOUR_OPENROUTER_KEY"
default_model = "anthropic/claude-sonnet-4"
enabled = true
priority = 80

# Grok (xAI, needs translation)
[[profiles]]
name = "grok"
provider_type = "OpenAICompatible"
base_url = "https://api.x.ai/v1"
api_key = "YOUR_XAI_KEY"
default_model = "grok-3-beta"
backup_providers = ["deepseek"]
enabled = true
priority = 100

# OpenAI (needs translation)
[[profiles]]
name = "chatgpt"
provider_type = "OpenAICompatible"
base_url = "https://api.openai.com/v1"
api_key = "YOUR_API_KEY"
default_model = "gpt-4o"
enabled = true
priority = 90

# DeepSeek (needs translation)
[[profiles]]
name = "deepseek"
provider_type = "OpenAICompatible"
base_url = "https://api.deepseek.com"
api_key = "YOUR_API_KEY"
default_model = "deepseek-chat"
backup_providers = ["grok"]
enabled = true
priority = 100

# Kimi / Moonshot (needs translation)
[[profiles]]
name = "kimi"
provider_type = "OpenAICompatible"
base_url = "https://api.moonshot.ai/v1"
api_key = "YOUR_API_KEY"
default_model = "kimi-k2-0905-preview"
enabled = true
priority = 70

# GLM / 智谱 (needs translation)
[[profiles]]
name = "glm"
provider_type = "OpenAICompatible"
base_url = "https://api.z.ai/api/paas/v4"
api_key = "YOUR_API_KEY"
default_model = "glm-4.6"
enabled = true
priority = 70

# ─── OAuth Subscription Profiles ───────────────────────
# Login first: claudex auth login <provider> --profile <name>

# Claude Max (via Claude Code's own OAuth)
[[profiles]]
name = "claude-max"
provider_type = "DirectAnthropic"
base_url = "https://api.claude.ai"
default_model = "claude-sonnet-4-20250514"
auth_type = "oauth"
oauth_provider = "claude"
enabled = false

# ChatGPT Plus/Pro / Codex subscription (via OpenAI OAuth → Responses API)
[[profiles]]
name = "codex-sub"
provider_type = "OpenAIResponses"
base_url = "https://chatgpt.com/backend-api/codex"
default_model = "gpt-4o"
auth_type = "oauth"
oauth_provider = "openai"
enabled = false
# ChatGPT-Account-ID is auto-extracted from Codex CLI's auth.json
# Or set manually: [profiles.custom_headers]
# ChatGPT-Account-ID = "your-account-id"

# Gemini Pro (via Google OAuth)
[[profiles]]
name = "gemini-sub"
provider_type = "OpenAICompatible"
base_url = "https://generativelanguage.googleapis.com/v1beta/openai"
default_model = "gemini-2.5-pro"
auth_type = "oauth"
oauth_provider = "google"
enabled = false

# GitHub Copilot (via GitHub Device Code Flow)
[[profiles]]
name = "copilot"
provider_type = "OpenAICompatible"
base_url = "https://api.githubcopilot.com"
default_model = "gpt-4o"
auth_type = "oauth"
oauth_provider = "github"
enabled = false

# ─── Local Models (via Ollama / vLLM / LM Studio) ──────

# Ollama
[[profiles]]
name = "local-qwen"
provider_type = "OpenAICompatible"
base_url = "http://localhost:11434/v1"
api_key = ""
default_model = "qwen2.5:72b"
enabled = false
priority = 50

# vLLM / llama.cpp server / LM Studio
[[profiles]]
name = "local-llama"
provider_type = "OpenAICompatible"
base_url = "http://localhost:8000/v1"
api_key = ""
default_model = "llama-3.3-70b"
enabled = false
priority = 50

# ─── Smart Router (optional) ───────────────────────────

[router]
enabled = false
profile = "local-qwen"              # reuse a profile's base_url + api_key for classification
model = "qwen2.5:3b"                # override model (optional, defaults to profile's default_model)

[router.rules]
code = "deepseek"
analysis = "grok"
creative = "chatgpt"
search = "kimi"
math = "deepseek"
default = "grok"

# ─── Context Engine (optional) ─────────────────────────

[context.compression]
enabled = false
threshold_tokens = 50000
keep_recent = 10
profile = "local-qwen"              # reuse a profile's base_url + api_key for summarization
model = "qwen2.5:3b"                # override model (optional)

[context.sharing]
enabled = false
max_context_size = 2000

[context.rag]
enabled = false
index_paths = ["./src", "./docs"]
profile = "local-qwen"              # reuse a profile's base_url + api_key for embeddings
model = "nomic-embed-text"           # embedding model
chunk_size = 512
top_k = 5
