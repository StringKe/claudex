---
title: コンテキストエンジン
description: 会話の圧縮、クロスプロファイル共有、ローカル RAG
---

import { Aside } from '@astrojs/starlight/components';

コンテキストエンジンは、会話の圧縮、クロスプロファイルのコンテキスト共有、ローカル RAG (検索拡張生成) の3つのメカニズムを通じて AI インタラクションを強化します。

<Aside type="caution">
  コンテキストエンジンの機能は要約とエンベディング生成のために LLM サービスが必要です。既存のプロファイル (ローカル Ollama、OpenRouter などのクラウドプロバイダー等) を使用できます。
</Aside>

## 会話の圧縮

会話がトークンしきい値を超えると、Claudex は LLM を使用して古いメッセージを要約し、最近のものをそのまま保持します。

```toml
[context.compression]
enabled = true
threshold_tokens = 50000    # 合計トークン数がこれを超えると圧縮
keep_recent = 10            # 常に最新 N 件のメッセージを保持
profile = "openrouter"      # プロファイルの base_url + api_key を再利用
model = "qwen/qwen-2.5-7b-instruct"  # モデルを上書き (任意)
```

### 仕組み

1. リクエストを転送する前に、Claudex が合計トークン数を推定する
2. トークン数が `threshold_tokens` を超えた場合、古いメッセージ (`keep_recent` を超えた分) が要約に置き換えられる
3. 要約は設定されたローカル LLM によって生成される
4. 圧縮された会話がプロバイダーに転送される

## クロスプロファイル共有

同じセッション内の異なるプロバイダープロファイル間でコンテキストを共有します。

```toml
[context.sharing]
enabled = true
max_context_size = 2000    # 他のプロファイルから注入する最大トークン数
```

これはタスクの途中でプロバイダーを切り替える際に有効です。以前のインタラクションからの関連コンテキストが自動的に含まれます。

## ローカル RAG

検索拡張生成のためにローカルコードとドキュメントをインデックス化します。関連するコードスニペットがリクエストに自動的に注入されます。

```toml
[context.rag]
enabled = true
index_paths = ["./src", "./docs"]     # インデックス化するディレクトリ
profile = "openrouter"                 # プロファイルの base_url + api_key を再利用
model = "openai/text-embedding-3-small"  # エンベディングモデル
chunk_size = 512                       # テキストチャンクサイズ
top_k = 5                             # 注入する結果数
```

### 仕組み

1. 起動時に Claudex がエンベディングモデルを使用して `index_paths` 内のファイルをインデックス化する
2. 各リクエストに対して、ユーザーのメッセージがエンベディングされてインデックスと比較される
3. 最も関連性の高い上位 K 件のチャンクがリクエストの追加コンテキストとして注入される
4. プロバイダーがコードベースについてより豊富なコンテキストを受け取る

<Aside type="tip">
  ローカル RAG の最良の結果を得るには、Ollama 経由の `nomic-embed-text` などの専用エンベディングモデルを使用してください。
</Aside>
