---
title: 設定リファレンス
description: Claudex のすべての設定オプションの完全なリファレンス
---

import { Aside } from '@astrojs/starlight/components';

## 設定ファイルの場所

Claudex は figment によるレイヤード設定を使用します。以下の順序でソースがマージされます（後のソースが前のソースを上書き）:

1. **プログラムデフォルト**
2. **グローバル設定**（`~/.config/claudex/config.toml` または `.yaml`）
3. **プロジェクト設定**（カレントディレクトリまたは親ディレクトリの `claudex.toml`/`.claudex/config.toml`、もしくは `$CLAUDEX_CONFIG`）
4. **環境変数**（`CLAUDEX_` プレフィックス、`__` セパレータ）

TOML と YAML の両フォーマットに対応しています。

詳細は[設定](/ja/configuration/)を参照してください。

## グローバル設定

```toml
# claude バイナリへのパス（デフォルト: PATH から "claude"）
claude_binary = "claude"

# プロキシサーバーのバインドポート
proxy_port = 13456

# プロキシサーバーのバインドアドレス
proxy_host = "127.0.0.1"

# ログレベル: trace, debug, info, warn, error
log_level = "info"

# ターミナルハイパーリンク（OSC 8）: "auto" | true | false
hyperlinks = "auto"
```

| フィールド | 型 | デフォルト | 説明 |
|-----------|-----|----------|------|
| `claude_binary` | string | `"claude"` | Claude Code CLI バイナリへのパス |
| `proxy_port` | integer | `13456` | 翻訳プロキシがリッスンするポート |
| `proxy_host` | string | `"127.0.0.1"` | プロキシがバインドするアドレス |
| `log_level` | string | `"info"` | 最小ログレベル |
| `hyperlinks` | string/bool | `"auto"` | ターミナルハイパーリンク: `"auto"`（検出）、`true`（強制有効）、`false`（強制無効） |

## モデルエイリアス

モデル識別子の短縮名を定義します:

```toml
[model_aliases]
grok3 = "grok-3-beta"
gpt4o = "gpt-4o"
ds3 = "deepseek-chat"
claude = "claude-sonnet-4-20250514"
```

`-m` でエイリアスを使用できます:

```bash
claudex run grok -m grok3
```

## プロファイル設定

```toml
[[profiles]]
name = "grok"
provider_type = "OpenAICompatible"
base_url = "https://api.x.ai/v1"
api_key = "xai-..."
# api_key_keyring = "grok-api-key"
default_model = "grok-3-beta"
auth_type = "api-key"              # "api-key"（デフォルト）または "oauth"
# oauth_provider = "openai"        # auth_type = "oauth" の場合に必須
backup_providers = ["deepseek"]
custom_headers = {}
extra_env = {}
priority = 100
enabled = true
max_tokens = 16384                  # オプション: 出力トークン数の上限
strip_params = "auto"               # "auto" | "none" | ["temperature", "top_p"]

# URL クエリパラメータ（例: Azure api-version）
[profiles.query_params]
# api-version = "2024-12-01-preview"

# モデルスロットマッピング（オプション）
[profiles.models]
haiku = "grok-3-mini-beta"
sonnet = "grok-3-beta"
opus = "grok-3-beta"
```

| フィールド | 型 | デフォルト | 説明 |
|-----------|-----|----------|------|
| `name` | string | *必須* | プロファイルの一意な識別子 |
| `provider_type` | string | `"DirectAnthropic"` | `"DirectAnthropic"`、`"OpenAICompatible"`、または `"OpenAIResponses"` |
| `base_url` | string | *必須* | プロバイダー API エンドポイント URL |
| `api_key` | string | `""` | プレーンテキストの API キー |
| `api_key_keyring` | string | -- | OS キーチェーンのエントリ名（`api_key` を上書き） |
| `default_model` | string | *必須* | デフォルトで使用するモデル識別子 |
| `auth_type` | string | `"api-key"` | 認証方法: `"api-key"` または `"oauth"` |
| `oauth_provider` | string | -- | OAuth プロバイダー名。`claude`、`openai`、`google`、`qwen`、`kimi`、`github`、`gitlab` のいずれか |
| `backup_providers` | string[] | `[]` | フェイルオーバー用プロファイル名（順番に試行） |
| `custom_headers` | map | `{}` | すべてのリクエストに送信する追加 HTTP ヘッダー |
| `extra_env` | map | `{}` | Claude 起動時に設定する環境変数 |
| `priority` | integer | `100` | スマートルーティングの優先度ウェイト（高いほど優先） |
| `enabled` | boolean | `true` | このプロファイルが有効かどうか |
| `max_tokens` | integer | -- | プロバイダーに送信する最大出力トークン数の上限。設定時、リクエストの `max_tokens` を上書き |
| `strip_params` | string/array | `"auto"` | リクエストから除去するパラメータ。`"auto"` は既知エンドポイントを検出、`"none"` はすべて送信、配列は指定パラメータを除去 |

### クエリパラメータ

```toml
[profiles.query_params]
api-version = "2024-12-01-preview"
```

| フィールド | 型 | 説明 |
|-----------|-----|------|
| `query_params` | map | すべてのリクエストの URL クエリパラメータとして付加されるキーバリューペア |

主に Azure OpenAI（`api-version`）で使用されますが、任意のプロバイダーで動作します。

### モデルスロットマッピング

オプションの `[profiles.models]` テーブルは Claude Code の `/model` スイッチャースロットをプロバイダー固有のモデル名にマッピングします。Claude Code 内でモデルを切り替えると（例: `/model opus`）、Claudex はリクエストをマッピングされたモデルに変換します。

```toml
[profiles.models]
haiku = "grok-3-mini-beta"    # /model haiku をマッピング
sonnet = "grok-3-beta"        # /model sonnet をマッピング
opus = "grok-3-beta"          # /model opus をマッピング
```

| フィールド | 型 | 説明 |
|-----------|-----|------|
| `haiku` | string | Claude Code が `haiku` を選択した際に使用するモデル |
| `sonnet` | string | Claude Code が `sonnet` を選択した際に使用するモデル |
| `opus` | string | Claude Code が `opus` を選択した際に使用するモデル |

<Aside type="tip">
  スロットが定義されていない場合、プロファイルの `default_model` がフォールバックとして使用されます。スロットは環境変数としても公開されます: `ANTHROPIC_DEFAULT_HAIKU_MODEL`、`ANTHROPIC_DEFAULT_SONNET_MODEL`、`ANTHROPIC_DEFAULT_OPUS_MODEL`。
</Aside>

### プロバイダー設定例

```toml
# Anthropic（DirectAnthropic -- 翻訳なし）
[[profiles]]
name = "anthropic"
provider_type = "DirectAnthropic"
base_url = "https://api.anthropic.com"
api_key = "sk-ant-..."
default_model = "claude-sonnet-4-20250514"

# MiniMax（DirectAnthropic -- 翻訳なし）
[[profiles]]
name = "minimax"
provider_type = "DirectAnthropic"
base_url = "https://api.minimax.io/anthropic"
api_key = "..."
default_model = "claude-sonnet-4-20250514"
backup_providers = ["anthropic"]

# OpenRouter（OpenAICompatible -- 翻訳が必要）
[[profiles]]
name = "openrouter"
provider_type = "OpenAICompatible"
base_url = "https://openrouter.ai/api/v1"
api_key = "..."
default_model = "anthropic/claude-sonnet-4"

# Grok（OpenAICompatible -- 翻訳が必要）
[[profiles]]
name = "grok"
provider_type = "OpenAICompatible"
base_url = "https://api.x.ai/v1"
api_key = "xai-..."
default_model = "grok-3-beta"
backup_providers = ["deepseek"]

# OpenAI（OpenAICompatible -- 翻訳が必要）
[[profiles]]
name = "chatgpt"
provider_type = "OpenAICompatible"
base_url = "https://api.openai.com/v1"
api_key = "sk-..."
default_model = "gpt-4o"

# DeepSeek（OpenAICompatible -- 翻訳が必要）
[[profiles]]
name = "deepseek"
provider_type = "OpenAICompatible"
base_url = "https://api.deepseek.com"
api_key = "..."
default_model = "deepseek-chat"
backup_providers = ["grok"]

# Kimi / Moonshot（OpenAICompatible -- 翻訳が必要）
[[profiles]]
name = "kimi"
provider_type = "OpenAICompatible"
base_url = "https://api.moonshot.ai/v1"
api_key = "..."
default_model = "kimi-k2-0905-preview"

# GLM / 智谱（OpenAICompatible -- 翻訳が必要）
[[profiles]]
name = "glm"
provider_type = "OpenAICompatible"
base_url = "https://api.z.ai/api/paas/v4"
api_key = "..."
default_model = "glm-4.6"

# Groq（OpenAICompatible -- 高速推論）
[[profiles]]
name = "groq"
provider_type = "OpenAICompatible"
base_url = "https://api.groq.com/openai/v1"
api_key = "gsk_..."
default_model = "llama-3.3-70b-versatile"

# Mistral AI（OpenAICompatible -- 翻訳が必要）
[[profiles]]
name = "mistral"
provider_type = "OpenAICompatible"
base_url = "https://api.mistral.ai/v1"
api_key = "..."
default_model = "mistral-large-latest"

# Together AI（OpenAICompatible -- 翻訳が必要）
[[profiles]]
name = "together"
provider_type = "OpenAICompatible"
base_url = "https://api.together.xyz/v1"
api_key = "..."
default_model = "meta-llama/Llama-3.3-70B-Instruct-Turbo"

# Perplexity（OpenAICompatible -- オンライン検索 + LLM）
[[profiles]]
name = "perplexity"
provider_type = "OpenAICompatible"
base_url = "https://api.perplexity.ai"
api_key = "pplx-..."
default_model = "sonar-pro"

# Cerebras（OpenAICompatible -- 高速推論）
[[profiles]]
name = "cerebras"
provider_type = "OpenAICompatible"
base_url = "https://api.cerebras.ai/v1"
api_key = "..."
default_model = "llama-3.3-70b"

# Azure OpenAI（OpenAICompatible -- api-key ヘッダー + query_params）
[[profiles]]
name = "azure-openai"
provider_type = "OpenAICompatible"
base_url = "https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT"
api_key = "YOUR_AZURE_KEY"
default_model = "gpt-4o"
[profiles.query_params]
api-version = "2024-12-01-preview"

# Google Vertex AI（DirectAnthropic）
[[profiles]]
name = "vertex-ai"
provider_type = "DirectAnthropic"
base_url = "https://us-east5-aiplatform.googleapis.com/v1/projects/YOUR_PROJECT/locations/us-east5/publishers/anthropic/models"
api_key = "YOUR_GCLOUD_TOKEN"
default_model = "claude-sonnet-4@20250514"

# Ollama（ローカル、API キー不要）
[[profiles]]
name = "local-qwen"
provider_type = "OpenAICompatible"
base_url = "http://localhost:11434/v1"
api_key = ""
default_model = "qwen2.5:72b"
enabled = false

# vLLM / LM Studio（ローカル）
[[profiles]]
name = "local-llama"
provider_type = "OpenAICompatible"
base_url = "http://localhost:8000/v1"
api_key = ""
default_model = "llama-3.3-70b"
enabled = false
```

### OAuth プロファイル設定例

```toml
# Claude Max（プロキシをスキップ、~/.claude のネイティブ OAuth を使用）
[[profiles]]
name = "claude-max"
provider_type = "DirectAnthropic"
base_url = "https://api.claude.ai"
default_model = "claude-sonnet-4-20250514"
auth_type = "oauth"
oauth_provider = "claude"

[profiles.models]
haiku = "claude-haiku-4-20250514"
sonnet = "claude-sonnet-4-20250514"
opus = "claude-opus-4-20250514"

# ChatGPT/Codex サブスクリプション（OpenAIResponses）
[[profiles]]
name = "codex-sub"
provider_type = "OpenAIResponses"
base_url = "https://chatgpt.com/backend-api/codex"
default_model = "gpt-5.3-codex"
auth_type = "oauth"
oauth_provider = "openai"

[profiles.models]
haiku = "codex-mini-latest"
sonnet = "gpt-5.3-codex"
opus = "gpt-5.3-codex"

# Google Gemini via OAuth
[[profiles]]
name = "gemini-sub"
provider_type = "OpenAICompatible"
base_url = "https://generativelanguage.googleapis.com/v1beta/openai"
default_model = "gemini-2.5-pro"
auth_type = "oauth"
oauth_provider = "google"

# Kimi via OAuth
[[profiles]]
name = "kimi-oauth"
provider_type = "OpenAICompatible"
base_url = "https://api.moonshot.cn/v1"
default_model = "moonshot-v1-128k"
auth_type = "oauth"
oauth_provider = "kimi"

# Qwen via OAuth
[[profiles]]
name = "qwen-oauth"
provider_type = "OpenAICompatible"
base_url = "https://chat.qwen.ai/api"
default_model = "qwen3-235b-a22b"
auth_type = "oauth"
oauth_provider = "qwen"

# GitHub Copilot via OAuth
[[profiles]]
name = "copilot"
provider_type = "OpenAICompatible"
base_url = "https://api.githubcopilot.com"
default_model = "gpt-4o"
auth_type = "oauth"
oauth_provider = "github"

# GitLab Duo via GITLAB_TOKEN
[[profiles]]
name = "gitlab-duo"
provider_type = "OpenAICompatible"
base_url = "https://gitlab.com/api/v4/ai/llm/proxy"
default_model = "claude-sonnet-4-20250514"
auth_type = "oauth"
oauth_provider = "gitlab"
```

<Aside type="caution">
  `claude` OAuth プロバイダーの場合、Claudex はプロキシをスキップし、Claude Code が自身の OAuth を直接使用できるようにします。その他すべての OAuth プロバイダーでは、プロキシがトークン管理とリフレッシュを処理します。
</Aside>

## スマートルーター

```toml
[router]
enabled = false
profile = "local-qwen"    # プロファイルの base_url + api_key を再利用
model = "qwen2.5:3b"      # モデルを上書き（任意）
```

| フィールド | 型 | デフォルト | 説明 |
|-----------|-----|----------|------|
| `enabled` | boolean | `false` | スマートルーティングを有効化 |
| `profile` | string | `""` | 分類に再利用するプロファイル名（`base_url` + `api_key` を使用） |
| `model` | string | `""` | 分類のモデル上書き（デフォルトはプロファイルの `default_model`） |

### ルーティングルール

```toml
[router.rules]
code = "deepseek"
analysis = "grok"
creative = "chatgpt"
search = "kimi"
math = "deepseek"
default = "grok"
```

| キー | 説明 |
|------|------|
| `code` | コーディングタスク用プロファイル |
| `analysis` | 分析と推論用プロファイル |
| `creative` | クリエイティブライティング用プロファイル |
| `search` | 検索とリサーチ用プロファイル |
| `math` | 数学と論理用プロファイル |
| `default` | インテントが未分類の場合のフォールバック |

## コンテキストエンジン

### 圧縮

```toml
[context.compression]
enabled = false
threshold_tokens = 50000
keep_recent = 10
profile = "local-qwen"    # プロファイルの base_url + api_key を再利用
model = "qwen2.5:3b"      # モデルを上書き（任意）
```

| フィールド | 型 | デフォルト | 説明 |
|-----------|-----|----------|------|
| `enabled` | boolean | `false` | 会話圧縮を有効化 |
| `threshold_tokens` | integer | `50000` | トークン数がこれを超えると圧縮 |
| `keep_recent` | integer | `10` | 常に最新 N 件のメッセージを非圧縮で保持 |
| `profile` | string | `""` | 要約に再利用するプロファイル名 |
| `model` | string | `""` | 要約のモデル上書き |

### プロファイル間共有

```toml
[context.sharing]
enabled = false
max_context_size = 2000
```

| フィールド | 型 | デフォルト | 説明 |
|-----------|-----|----------|------|
| `enabled` | boolean | `false` | プロファイル間コンテキスト共有を有効化 |
| `max_context_size` | integer | `2000` | 他のプロファイルから注入する最大トークン数 |

### ローカル RAG

```toml
[context.rag]
enabled = false
index_paths = ["./src", "./docs"]
profile = "local-qwen"              # プロファイルの base_url + api_key を再利用
model = "nomic-embed-text"           # エンベディングモデル
chunk_size = 512
top_k = 5
```

| フィールド | 型 | デフォルト | 説明 |
|-----------|-----|----------|------|
| `enabled` | boolean | `false` | ローカル RAG を有効化 |
| `index_paths` | string[] | `[]` | インデックス化するディレクトリ |
| `profile` | string | `""` | エンベディングに再利用するプロファイル名 |
| `model` | string | `""` | エンベディングモデル名 |
| `chunk_size` | integer | `512` | トークン単位のテキストチャンクサイズ |
| `top_k` | integer | `5` | 注入する結果数 |
