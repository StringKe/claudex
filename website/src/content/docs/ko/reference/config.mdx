---
title: 설정 레퍼런스
description: 모든 Claudex 설정 옵션의 완전한 레퍼런스
---

import { Aside } from '@astrojs/starlight/components';

## 설정 파일 위치

Claudex는 다음 순서로 설정 파일을 탐색합니다:

1. `$CLAUDEX_CONFIG` 환경 변수
2. `./claudex.toml` (현재 디렉토리)
3. `./.claudex/config.toml` (현재 디렉토리)
4. 상위 디렉토리 (최대 10단계), 두 패턴 모두 확인
5. `~/.config/claudex/config.toml` (XDG — 플랫폼별 경로보다 **먼저** 확인)

자세한 내용은 [설정](/ko/configuration/)을 참조하세요.

## 전역 설정

```toml
# claude 바이너리 경로 (기본값: PATH에서 "claude")
claude_binary = "claude"

# 프록시 서버 바인드 포트
proxy_port = 13456

# 프록시 서버 바인드 주소
proxy_host = "127.0.0.1"

# 로그 레벨: trace, debug, info, warn, error
log_level = "info"
```

| 필드 | 타입 | 기본값 | 설명 |
|-------|------|---------|-------------|
| `claude_binary` | string | `"claude"` | Claude Code CLI 바이너리 경로 |
| `proxy_port` | integer | `13456` | 번역 프록시가 수신하는 포트 |
| `proxy_host` | string | `"127.0.0.1"` | 프록시가 바인딩되는 주소 |
| `log_level` | string | `"info"` | 최소 로그 레벨 |

## 모델 별칭

모델 식별자의 단축명을 정의합니다:

```toml
[model_aliases]
grok3 = "grok-3-beta"
gpt4o = "gpt-4o"
ds3 = "deepseek-chat"
claude = "claude-sonnet-4-20250514"
```

`-m`으로 별칭 사용:

```bash
claudex run grok -m grok3
```

## 프로파일 설정

```toml
[[profiles]]
name = "grok"
provider_type = "OpenAICompatible"
base_url = "https://api.x.ai/v1"
api_key = "xai-..."
# api_key_keyring = "grok-api-key"
default_model = "grok-3-beta"
auth_type = "api-key"              # "api-key" (기본값) 또는 "oauth"
# oauth_provider = "openai"        # auth_type = "oauth"일 때 필수
backup_providers = ["deepseek"]
custom_headers = {}
extra_env = {}
priority = 100
enabled = true

# 모델 슬롯 매핑 (선택 사항)
[profiles.models]
haiku = "grok-3-mini-beta"
sonnet = "grok-3-beta"
opus = "grok-3-beta"
```

| 필드 | 타입 | 기본값 | 설명 |
|-------|------|---------|-------------|
| `name` | string | *필수* | 고유한 프로파일 식별자 |
| `provider_type` | string | `"DirectAnthropic"` | `"DirectAnthropic"`, `"OpenAICompatible"`, 또는 `"OpenAIResponses"` |
| `base_url` | string | *필수* | 제공자 API 엔드포인트 URL |
| `api_key` | string | `""` | 평문 API 키 |
| `api_key_keyring` | string | — | OS 키체인 항목 이름 (`api_key` 오버라이드) |
| `default_model` | string | *필수* | 기본으로 사용할 모델 식별자 |
| `auth_type` | string | `"api-key"` | 인증 방식: `"api-key"` 또는 `"oauth"` |
| `oauth_provider` | string | — | OAuth 제공자 이름 (`auth_type = "oauth"`일 때 필수). `claude`, `openai`, `google`, `qwen`, `kimi`, `github` 중 하나 |
| `backup_providers` | string[] | `[]` | 장애 조치용 프로파일 이름 목록, 순서대로 시도 |
| `custom_headers` | map | `{}` | 모든 요청에 전송할 추가 HTTP 헤더 |
| `extra_env` | map | `{}` | Claude 실행 시 설정할 환경 변수 |
| `priority` | integer | `100` | 스마트 라우팅 우선순위 가중치 (높을수록 우선) |
| `enabled` | boolean | `true` | 이 프로파일의 활성화 여부 |

### 모델 슬롯 매핑

선택적 `[profiles.models]` 테이블은 Claude Code의 `/model` 전환기 슬롯을 제공자별 모델 이름에 매핑합니다. Claude Code 내에서 모델을 전환하면(예: `/model opus`), Claudex가 매핑된 모델을 사용하도록 요청을 변환합니다.

```toml
[profiles.models]
haiku = "grok-3-mini-beta"    # /model haiku 매핑
sonnet = "grok-3-beta"        # /model sonnet 매핑
opus = "grok-3-beta"          # /model opus 매핑
```

| 필드 | 타입 | 설명 |
|-------|------|-------------|
| `haiku` | string | Claude Code에서 `haiku` 선택 시 사용할 모델 |
| `sonnet` | string | Claude Code에서 `sonnet` 선택 시 사용할 모델 |
| `opus` | string | Claude Code에서 `opus` 선택 시 사용할 모델 |

<Aside type="tip">
  슬롯이 정의되지 않은 경우, 프로파일의 `default_model`이 폴백으로 사용됩니다.
</Aside>

### 제공자 예시

```toml
# Anthropic (DirectAnthropic — 변환 없음)
[[profiles]]
name = "anthropic"
provider_type = "DirectAnthropic"
base_url = "https://api.anthropic.com"
api_key = "sk-ant-..."
default_model = "claude-sonnet-4-20250514"

# MiniMax (DirectAnthropic — 변환 없음)
[[profiles]]
name = "minimax"
provider_type = "DirectAnthropic"
base_url = "https://api.minimax.io/anthropic"
api_key = "..."
default_model = "claude-sonnet-4-20250514"
backup_providers = ["anthropic"]

# OpenRouter (OpenAICompatible — 변환 필요)
[[profiles]]
name = "openrouter"
provider_type = "OpenAICompatible"
base_url = "https://openrouter.ai/api/v1"
api_key = "..."
default_model = "anthropic/claude-sonnet-4"

# Grok (OpenAICompatible — 변환 필요)
[[profiles]]
name = "grok"
provider_type = "OpenAICompatible"
base_url = "https://api.x.ai/v1"
api_key = "xai-..."
default_model = "grok-3-beta"
backup_providers = ["deepseek"]

# OpenAI (OpenAICompatible — 변환 필요)
[[profiles]]
name = "chatgpt"
provider_type = "OpenAICompatible"
base_url = "https://api.openai.com/v1"
api_key = "sk-..."
default_model = "gpt-4o"

# DeepSeek (OpenAICompatible — 변환 필요)
[[profiles]]
name = "deepseek"
provider_type = "OpenAICompatible"
base_url = "https://api.deepseek.com"
api_key = "..."
default_model = "deepseek-chat"
backup_providers = ["grok"]

# Kimi / Moonshot (OpenAICompatible — 변환 필요)
[[profiles]]
name = "kimi"
provider_type = "OpenAICompatible"
base_url = "https://api.moonshot.cn/v1"
api_key = "..."
default_model = "moonshot-v1-128k"

# GLM / 智谱 (OpenAICompatible — 변환 필요)
[[profiles]]
name = "glm"
provider_type = "OpenAICompatible"
base_url = "https://open.bigmodel.cn/api/paas/v4"
api_key = "..."
default_model = "glm-4-plus"

# Ollama (로컬, API 키 불필요)
[[profiles]]
name = "local-qwen"
provider_type = "OpenAICompatible"
base_url = "http://localhost:11434/v1"
api_key = ""
default_model = "qwen2.5:72b"
enabled = false

# vLLM / LM Studio (로컬)
[[profiles]]
name = "local-llama"
provider_type = "OpenAICompatible"
base_url = "http://localhost:8000/v1"
api_key = ""
default_model = "llama-3.3-70b"
enabled = false

# ChatGPT/Codex 구독 (OpenAIResponses — Responses API 변환)
[[profiles]]
name = "codex-sub"
provider_type = "OpenAIResponses"
base_url = "https://chatgpt.com/backend-api/codex"
default_model = "gpt-4o"
auth_type = "oauth"
oauth_provider = "openai"
```

### OAuth 프로파일 예시

```toml
# OAuth를 통한 OpenAI (Codex CLI ~/.codex/auth.json에서 토큰 읽기)
[[profiles]]
name = "chatgpt-oauth"
provider_type = "OpenAICompatible"
base_url = "https://api.openai.com/v1"
default_model = "gpt-4o"
auth_type = "oauth"
oauth_provider = "openai"

[profiles.models]
haiku = "gpt-4o-mini"
sonnet = "gpt-4o"
opus = "o1"

# Claude 구독 (프록시 건너뜀, ~/.claude의 Claude 네이티브 OAuth 사용)
[[profiles]]
name = "claude-sub"
provider_type = "DirectAnthropic"
base_url = "https://api.anthropic.com"
default_model = "claude-sonnet-4-20250514"
auth_type = "oauth"
oauth_provider = "claude"

[profiles.models]
haiku = "claude-haiku-4-20250514"
sonnet = "claude-sonnet-4-20250514"
opus = "claude-opus-4-20250514"

# OAuth를 통한 Google Gemini
[[profiles]]
name = "gemini"
provider_type = "OpenAICompatible"
base_url = "https://generativelanguage.googleapis.com/v1beta/openai"
default_model = "gemini-2.5-pro"
auth_type = "oauth"
oauth_provider = "google"

# OAuth를 통한 Kimi
[[profiles]]
name = "kimi-oauth"
provider_type = "OpenAICompatible"
base_url = "https://api.moonshot.cn/v1"
default_model = "moonshot-v1-128k"
auth_type = "oauth"
oauth_provider = "kimi"

# OAuth를 통한 Qwen
[[profiles]]
name = "qwen-oauth"
provider_type = "OpenAICompatible"
base_url = "https://chat.qwenlm.ai/api/chat/v1"
default_model = "qwen-max"
auth_type = "oauth"
oauth_provider = "qwen"

# OAuth를 통한 GitHub Copilot
[[profiles]]
name = "github-copilot"
provider_type = "OpenAICompatible"
base_url = "https://api.githubcopilot.com"
default_model = "gpt-4o"
auth_type = "oauth"
oauth_provider = "github"

# OAuth를 통한 ChatGPT/Codex 구독 (OpenAIResponses)
[[profiles]]
name = "codex-sub"
provider_type = "OpenAIResponses"
base_url = "https://chatgpt.com/backend-api/codex"
default_model = "gpt-4o"
auth_type = "oauth"
oauth_provider = "openai"

[profiles.models]
haiku = "gpt-4o-mini"
sonnet = "gpt-4o"
opus = "o1-pro"
```

<Aside type="caution">
  `claude` OAuth 제공자의 경우, Claudex는 특수 게이트웨이 인증 모드를 사용합니다: Claude Code 자체 구독 로그인 플로우와의 충돌을 피하기 위해 `ANTHROPIC_API_KEY` 대신 `ANTHROPIC_AUTH_TOKEN`을 설정합니다.
</Aside>

## 스마트 라우터

```toml
[router]
enabled = false
profile = "local-qwen"    # 프로파일의 base_url + api_key 재사용
model = "qwen2.5:3b"      # 모델 오버라이드 (선택 사항)
```

| 필드 | 타입 | 기본값 | 설명 |
|-------|------|---------|-------------|
| `enabled` | boolean | `false` | 스마트 라우팅 활성화 |
| `profile` | string | `""` | 분류에 재사용할 프로파일 이름 (해당 `base_url` + `api_key` 사용) |
| `model` | string | `""` | 분류를 위한 모델 오버라이드 (기본값은 프로파일의 `default_model`) |

### 라우팅 규칙

```toml
[router.rules]
code = "deepseek"
analysis = "grok"
creative = "chatgpt"
search = "kimi"
math = "deepseek"
default = "grok"
```

| 키 | 설명 |
|-----|-------------|
| `code` | 코딩 작업용 프로파일 |
| `analysis` | 분석 및 추론용 프로파일 |
| `creative` | 창작 작업용 프로파일 |
| `search` | 검색 및 조사용 프로파일 |
| `math` | 수학 및 논리용 프로파일 |
| `default` | 의도 미분류 시 폴백 |

## 컨텍스트 엔진

### 압축

```toml
[context.compression]
enabled = false
threshold_tokens = 50000
keep_recent = 10
profile = "local-qwen"    # 프로파일의 base_url + api_key 재사용
model = "qwen2.5:3b"      # 모델 오버라이드 (선택 사항)
```

| 필드 | 타입 | 기본값 | 설명 |
|-------|------|---------|-------------|
| `enabled` | boolean | `false` | 대화 압축 활성화 |
| `threshold_tokens` | integer | `50000` | 토큰 수가 이를 초과하면 압축 |
| `keep_recent` | integer | `10` | 항상 마지막 N개의 메시지를 압축하지 않고 유지 |
| `profile` | string | `""` | 요약에 재사용할 프로파일 이름 (해당 `base_url` + `api_key` 사용) |
| `model` | string | `""` | 요약을 위한 모델 오버라이드 (기본값은 프로파일의 `default_model`) |

### 프로파일 간 공유

```toml
[context.sharing]
enabled = false
max_context_size = 2000
```

| 필드 | 타입 | 기본값 | 설명 |
|-------|------|---------|-------------|
| `enabled` | boolean | `false` | 프로파일 간 컨텍스트 공유 활성화 |
| `max_context_size` | integer | `2000` | 다른 프로파일에서 주입할 최대 토큰 수 |

### 로컬 RAG

```toml
[context.rag]
enabled = false
index_paths = ["./src", "./docs"]
profile = "local-qwen"              # 프로파일의 base_url + api_key 재사용
model = "nomic-embed-text"           # 임베딩 모델
chunk_size = 512
top_k = 5
```

| 필드 | 타입 | 기본값 | 설명 |
|-------|------|---------|-------------|
| `enabled` | boolean | `false` | 로컬 RAG 활성화 |
| `index_paths` | string[] | `[]` | 인덱싱할 디렉토리 목록 |
| `profile` | string | `""` | 임베딩에 재사용할 프로파일 이름 (해당 `base_url` + `api_key` 사용) |
| `model` | string | `""` | 임베딩 모델 이름 (기본값은 프로파일의 `default_model`) |
| `chunk_size` | integer | `512` | 토큰 단위 텍스트 청크 크기 |
| `top_k` | integer | `5` | 주입할 결과 수 |
