---
title: Proxy de traduction
description: Traduction automatique du protocole Anthropic vers OpenAI
---

import { Aside } from '@astrojs/starlight/components';

Le proxy de traduction est le cœur de Claudex. Il s'intercale entre Claude Code et vos fournisseurs d'IA, convertissant de manière transparente entre l'API Anthropic Messages et l'API OpenAI Chat Completions.

## Comment ça fonctionne

```
Claude Code → requête API Anthropic Messages
    │
    └── Proxy Claudex (127.0.0.1:13456)
        │
        ├── Fournisseur DirectAnthropic → transmission avec en-têtes
        │
        ├── Fournisseur OpenAICompatible
        │   ├── Traduire la requête : Anthropic → OpenAI Chat Completions
        │   ├── Transmettre au fournisseur
        │   └── Traduire la réponse : OpenAI → Anthropic
        │
        └── Fournisseur OpenAIResponses
            ├── Traduire la requête : Anthropic → API OpenAI Responses
            ├── Transmettre au fournisseur
            └── Traduire la réponse : Responses → Anthropic
```

## Ce qui est traduit

### Traduction des requêtes (Anthropic → OpenAI)

| Anthropic | OpenAI |
|-----------|--------|
| Champ `system` | Message système dans le tableau `messages` |
| Blocs `messages[].content` (text, image, tool_use) | `messages[].content` + `tool_calls` |
| Tableau `tools` (JSON Schema) | Tableau `tools` (format function) |
| `tool_choice` | `tool_choice` |
| `max_tokens` | `max_tokens` |
| `temperature`, `top_p` | Correspondance directe |

### Traduction des réponses (OpenAI → Anthropic)

| OpenAI | Anthropic |
|--------|-----------|
| `choices[0].message.content` | Blocs `content` |
| `choices[0].message.tool_calls` | Blocs de contenu `tool_use` |
| `finish_reason: stop` | `stop_reason: end_turn` |
| `finish_reason: tool_calls` | `stop_reason: tool_use` |
| `usage.prompt_tokens` / `completion_tokens` | `usage.input_tokens` / `output_tokens` |

## Traduction en streaming

Claudex prend entièrement en charge le streaming SSE (Server-Sent Events), traduisant les chunks de flux OpenAI en événements de flux Anthropic en temps réel :

| SSE OpenAI | SSE Anthropic |
|------------|---------------|
| Premier chunk | `message_start` + `content_block_start` |
| `choices[0].delta.content` | `content_block_delta` (text_delta) |
| `choices[0].delta.tool_calls` | `content_block_delta` (input_json_delta) |
| `finish_reason` présent | `content_block_stop` + `message_delta` + `message_stop` |

Le traducteur en streaming maintient une machine d'état pour gérer correctement l'accumulation des appels d'outils et les limites des blocs de contenu.

## Fournisseurs pris en charge

| Fournisseur | Type | URL de base |
|-------------|------|-------------|
| Anthropic | DirectAnthropic | `https://api.anthropic.com` |
| MiniMax | DirectAnthropic | `https://api.minimax.io/anthropic` |
| OpenRouter | OpenAICompatible | `https://openrouter.ai/api/v1` |
| Grok (xAI) | OpenAICompatible | `https://api.x.ai/v1` |
| OpenAI | OpenAICompatible | `https://api.openai.com/v1` |
| DeepSeek | OpenAICompatible | `https://api.deepseek.com` |
| Kimi/Moonshot | OpenAICompatible | `https://api.moonshot.cn/v1` |
| GLM (Zhipu) | OpenAICompatible | `https://open.bigmodel.cn/api/paas/v4` |
| Ollama | OpenAICompatible | `http://localhost:11434/v1` |
| vLLM | OpenAICompatible | `http://localhost:8000/v1` |
| ChatGPT/Codex sub | OpenAIResponses | `https://chatgpt.com/backend-api/codex` |

<Aside type="tip">
  Les fournisseurs DirectAnthropic ne nécessitent aucune traduction. Les fournisseurs OpenAIResponses utilisent l'API Responses (point de terminaison `/responses`) au lieu de Chat Completions (`/chat/completions`).
</Aside>

## Gestion du proxy

```bash
# Démarrer le proxy en tant que démon
claudex proxy start -d

# Vérifier le statut du proxy
claudex proxy status

# Arrêter le démon proxy
claudex proxy stop

# Démarrer sur un port personnalisé
claudex proxy start -p 8080
```

Lorsque vous exécutez `claudex run <profile>`, le proxy est automatiquement démarré en arrière-plan s'il ne tourne pas déjà.
