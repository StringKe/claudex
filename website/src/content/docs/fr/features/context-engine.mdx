---
title: Moteur de contexte
description: Compression de conversation, partage inter-profils, et RAG local
---

import { Aside } from '@astrojs/starlight/components';

Le moteur de contexte ameliore vos interactions IA grace a trois mecanismes : la compression de conversation, le partage de contexte inter-profils, et le RAG local (Retrieval-Augmented Generation).

<Aside type="caution">
  Les fonctionnalites du moteur de contexte necessitent un service LLM pour la synthese et la generation d'embeddings. Vous pouvez utiliser n'importe quel profil existant (Ollama local, fournisseurs cloud comme OpenRouter, etc.).
</Aside>

## Compression de conversation

Lorsque les conversations depassent un seuil de tokens, Claudex utilise un LLM pour resumer les messages plus anciens, en conservant les plus recents intacts.

```toml
[context.compression]
enabled = true
threshold_tokens = 50000    # compresser quand le total de tokens depasse ce seuil
keep_recent = 10            # toujours conserver les N derniers messages
profile = "openrouter"      # reutiliser le base_url + api_key d'un profil
model = "qwen/qwen-2.5-7b-instruct"  # remplacer le modele (optionnel)
```

### Fonctionnement

1. Avant de transmettre une requete, Claudex estime le nombre total de tokens
2. Si les tokens depassent `threshold_tokens`, les messages plus anciens (au-dela de `keep_recent`) sont remplaces par un resume
3. Le resume est genere par le LLM local configure
4. La conversation compressee est ensuite transmise au fournisseur

## Partage inter-profils

Partagez le contexte entre differents profils de fournisseurs au sein de la meme session.

```toml
[context.sharing]
enabled = true
max_context_size = 2000    # nombre maximum de tokens a injecter depuis d'autres profils
```

Utile lorsque vous changez de fournisseur en cours de tache : le contexte pertinent des interactions precedentes est automatiquement inclus.

## RAG local

Indexez le code et la documentation locaux pour la generation augmentee par recuperation. Les extraits de code pertinents sont automatiquement injectes dans les requetes.

```toml
[context.rag]
enabled = true
index_paths = ["./src", "./docs"]     # repertoires a indexer
profile = "openrouter"                 # reutiliser le base_url + api_key d'un profil
model = "openai/text-embedding-3-small"  # modele d'embedding
chunk_size = 512                       # taille des chunks de texte
top_k = 5                             # nombre de resultats a injecter
```

### Fonctionnement

1. Au demarrage, Claudex indexe les fichiers dans `index_paths` en utilisant le modele d'embedding
2. Pour chaque requete, le message de l'utilisateur est transforme en embedding et compare a l'index
3. Les top-k chunks les plus pertinents sont injectes comme contexte supplementaire dans la requete
4. Le fournisseur recoit un contexte plus riche sur votre base de code

<Aside type="tip">
  Pour de meilleurs resultats avec le RAG local, utilisez un modele d'embedding dedie comme `nomic-embed-text` via Ollama.
</Aside>
