---
title: 上下文引擎
description: 對話壓縮、跨 profile 共享與本地 RAG
---

import { Aside } from '@astrojs/starlight/components';

上下文引擎透過三種機制增強你的 AI 互動：對話壓縮、跨 profile 上下文共享，以及本地 RAG（檢索增強生成）。

<Aside type="caution">
  上下文引擎功能需要 LLM 服務進行摘要生成和嵌入向量生成。你可以使用任何現有 profile（本地 Ollama、雲端提供商如 OpenRouter 等）。
</Aside>

## 對話壓縮

當對話超過 token 閾值時，Claudex 使用 LLM 對較舊的訊息進行摘要，保留最近的訊息完整。

```toml
[context.compression]
enabled = true
threshold_tokens = 50000    # 總 token 數超過此值時壓縮
keep_recent = 10            # 始終保留最後 N 條訊息
profile = "openrouter"      # 重用某個 profile 的 base_url + api_key
model = "qwen/qwen-2.5-7b-instruct"  # 覆寫模型（選用）
```

### 運作原理

1. 轉發請求前，Claudex 估算總 token 數
2. 若 token 數超過 `threshold_tokens`，超出 `keep_recent` 的較舊訊息會被替換為摘要
3. 摘要由設定的本地 LLM 生成
4. 壓縮後的對話再轉發至提供商

## 跨 Profile 共享

在同一工作階段中跨不同提供商 profile 共享上下文。

```toml
[context.sharing]
enabled = true
max_context_size = 2000    # 從其他 profile 注入的最大 token 數
```

這在任務中途切換提供商時很有用，來自先前互動的相關上下文會自動包含進來。

## 本地 RAG

索引本地程式碼和文件以進行檢索增強生成。相關程式碼片段會自動注入請求中。

```toml
[context.rag]
enabled = true
index_paths = ["./src", "./docs"]     # 要索引的目錄
profile = "openrouter"                 # 重用某個 profile 的 base_url + api_key
model = "openai/text-embedding-3-small"  # 嵌入模型
chunk_size = 512                       # 文字區塊大小
top_k = 5                             # 要注入的結果數量
```

### 運作原理

1. 啟動時，Claudex 使用嵌入模型為 `index_paths` 中的檔案建立索引
2. 對於每個請求，使用者訊息會被嵌入並與索引比對
3. 最相關的前 k 個區塊作為額外上下文注入請求
4. 提供商收到關於你程式碼庫更豐富的上下文

<Aside type="tip">
  為了取得最佳的本地 RAG 效果，建議透過 Ollama 使用專用嵌入模型，如 `nomic-embed-text`。
</Aside>
