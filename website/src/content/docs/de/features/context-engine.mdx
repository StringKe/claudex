---
title: Kontext-Engine
description: Konversationskomprimierung, profiluebergreifende Freigabe und lokales RAG
---

import { Aside } from '@astrojs/starlight/components';

Die Kontext-Engine verbessert KI-Interaktionen durch drei Mechanismen: Konversationskomprimierung, profiluebergreifende Kontextfreigabe und lokales RAG (Retrieval-Augmented Generation).

<Aside type="caution">
  Die Funktionen der Kontext-Engine benoetigen einen LLM-Dienst zur Zusammenfassung und Einbettungsgenerierung. Es kann jedes vorhandene Profil verwendet werden (lokales Ollama, Cloud-Anbieter wie OpenRouter usw.).
</Aside>

## Konversationskomprimierung

Wenn Konversationen einen Token-Schwellenwert ueberschreiten, verwendet Claudex ein LLM, um aeltere Nachrichten zusammenzufassen, waehrend neuere unveraendert bleiben.

```toml
[context.compression]
enabled = true
threshold_tokens = 50000    # komprimieren, wenn Gesamttoken diesen Wert ueberschreiten
keep_recent = 10            # immer die letzten N Nachrichten behalten
profile = "openrouter"      # base_url + api_key eines Profils wiederverwenden
model = "qwen/qwen-2.5-7b-instruct"  # Modell ueberschreiben (optional)
```

### Funktionsweise

1. Vor dem Weiterleiten einer Anfrage schaetzt Claudex die Gesamttoken-Anzahl
2. Wenn Token `threshold_tokens` ueberschreiten, werden aeltere Nachrichten (jenseits von `keep_recent`) durch eine Zusammenfassung ersetzt
3. Die Zusammenfassung wird vom konfigurierten lokalen LLM generiert
4. Die komprimierte Konversation wird dann an den Anbieter weitergeleitet

## Profiluebergreifende Freigabe

Kontext ueber verschiedene Anbieterprofile innerhalb derselben Sitzung teilen.

```toml
[context.sharing]
enabled = true
max_context_size = 2000    # maximale Token, die aus anderen Profilen injiziert werden
```

Dies ist nuetzlich beim Wechseln zwischen Anbietern waehrend einer Aufgabe. Relevanter Kontext aus frueheren Interaktionen wird automatisch einbezogen.

## Lokales RAG

Lokalen Code und Dokumentation fuer Retrieval-Augmented Generation indizieren. Relevante Code-Ausschnitte werden automatisch in Anfragen injiziert.

```toml
[context.rag]
enabled = true
index_paths = ["./src", "./docs"]     # zu indizierende Verzeichnisse
profile = "openrouter"                 # base_url + api_key eines Profils wiederverwenden
model = "openai/text-embedding-3-small"  # Einbettungsmodell
chunk_size = 512                       # Textchunk-Groesse
top_k = 5                             # Anzahl der zu injizierenden Ergebnisse
```

### Funktionsweise

1. Beim Start indiziert Claudex Dateien in `index_paths` mit dem Einbettungsmodell
2. Fuer jede Anfrage wird die Nachricht des Benutzers eingebettet und mit dem Index verglichen
3. Die top-k relevantesten Chunks werden als zusaetzlicher Kontext in die Anfrage injiziert
4. Der Anbieter erhaelt reichhaltigeren Kontext ueber die Codebasis

<Aside type="tip">
  Fuer beste Ergebnisse mit lokalem RAG ein dediziertes Einbettungsmodell wie `nomic-embed-text` ueber Ollama verwenden.
</Aside>
