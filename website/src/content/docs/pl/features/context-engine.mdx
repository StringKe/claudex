---
title: Silnik kontekstu
description: Kompresja konwersacji, udostepnianie kontekstu miedzy profilami i lokalny RAG
---

import { Aside } from '@astrojs/starlight/components';

Silnik kontekstu wzbogaca interakcje z AI przez trzy mechanizmy: kompresje konwersacji, udostepnianie kontekstu miedzy profilami i lokalny RAG (Retrieval-Augmented Generation).

<Aside type="caution">
  Funkcje silnika kontekstu wymagaja uslugi LLM do generowania podsumowani i osadzen. Mozesz uzyc dowolnego istniejacego profilu (lokalny Ollama, dostawcy chmurowi jak OpenRouter itp.).
</Aside>

## Kompresja konwersacji

Gdy konwersacje przekrocza prog tokenow, Claudex uzywa LLM do podsumowania starszych wiadomosci, zachowujac nienaruszone najnowsze.

```toml
[context.compression]
enabled = true
threshold_tokens = 50000    # kompresuj, gdy laczna liczba tokenow przekroczy te wartosc
keep_recent = 10            # zawsze zachowuj ostatnie N wiadomosci
profile = "openrouter"      # ponowne uzycie base_url + api_key profilu
model = "qwen/qwen-2.5-7b-instruct"  # nadpisanie modelu (opcjonalne)
```

### Jak to dziala

1. Przed przekazaniem zadania Claudex szacuje laczna liczbe tokenow
2. Jesli tokeny przekrocza `threshold_tokens`, starsze wiadomosci (poza `keep_recent`) sa zastepowane podsumowaniem
3. Podsumowanie jest generowane przez skonfigurowany lokalny LLM
4. Skompresowana konwersacja jest nastepnie przekazywana do dostawcy

## Udostepnianie kontekstu miedzy profilami

Udostepniaj kontekst miedzy roznymi profilami dostawcow w ramach tej samej sesji.

```toml
[context.sharing]
enabled = true
max_context_size = 2000    # maksymalna liczba tokenow do wstrzykniecia z innych profili
```

Jest to przydatne podczas przelaczania sie miedzy dostawcami w trakcie zadania â€” odpowiedni kontekst z poprzednich interakcji jest automatycznie dolaczany.

## Lokalny RAG

Indeksuj lokalne kody i dokumentacje do generowania wspomaganego pobieraniem. Odpowiednie fragmenty kodu sa automatycznie wstrzykiwane do zadan.

```toml
[context.rag]
enabled = true
index_paths = ["./src", "./docs"]     # katalogi do indeksowania
profile = "openrouter"                 # ponowne uzycie base_url + api_key profilu
model = "openai/text-embedding-3-small"  # model osadzen
chunk_size = 512                       # rozmiar fragmentu tekstu
top_k = 5                             # liczba wynikow do wstrzykniecia
```

### Jak to dziala

1. Przy uruchomieniu Claudex indeksuje pliki w `index_paths` przy uzyciu modelu osadzen
2. Dla kazdego zadania wiadomosc uzytkownika jest osadzana i porownywana z indeksem
3. Najlepsze k najbardziej trafnych fragmentow jest wstrzykiwanych jako dodatkowy kontekst w zadaniu
4. Dostawca otrzymuje bogatszy kontekst o bazie kodu

<Aside type="tip">
  Aby uzyskac najlepsze wyniki z lokalnym RAG, uzyj dedykowanego modelu osadzen, takiego jak `nomic-embed-text` przez Ollama.
</Aside>
