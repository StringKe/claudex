---
title: Справочник конфигурации
description: Полный справочник всех параметров конфигурации Claudex
---

import { Aside } from '@astrojs/starlight/components';

## Расположение файла конфигурации

Claudex использует многоуровневую конфигурацию через figment. Источники объединяются в следующем порядке (более поздние переопределяют более ранние):

1. **Программные значения по умолчанию**
2. **Глобальная конфигурация** (`~/.config/claudex/config.toml` или `.yaml`)
3. **Конфигурация проекта** (`claudex.toml`/`.claudex/config.toml` в текущем каталоге или родительских, либо `$CLAUDEX_CONFIG`)
4. **Переменные окружения** (префикс `CLAUDEX_`, разделитель `__`)

Поддерживаются оба формата: TOML и YAML.

Подробности настройки см. в разделе [Конфигурация](/ru/configuration/).

## Глобальные настройки

```toml
# Путь к бинарному файлу claude (по умолчанию: "claude" из PATH)
claude_binary = "claude"

# Порт привязки прокси-сервера
proxy_port = 13456

# Адрес привязки прокси-сервера
proxy_host = "127.0.0.1"

# Уровень логирования: trace, debug, info, warn, error
log_level = "info"

# Терминальные гиперссылки (OSC 8): "auto" | true | false
hyperlinks = "auto"
```

| Поле | Тип | По умолчанию | Описание |
|------|-----|--------------|---------|
| `claude_binary` | string | `"claude"` | Путь к бинарному файлу Claude Code CLI |
| `proxy_port` | integer | `13456` | Порт, на котором слушает прокси-переводчик |
| `proxy_host` | string | `"127.0.0.1"` | Адрес привязки прокси |
| `log_level` | string | `"info"` | Минимальный уровень логирования |
| `hyperlinks` | string/bool | `"auto"` | Терминальные гиперссылки: `"auto"` (определять), `true` (включить), `false` (выключить) |

## Алиасы моделей

Определите сокращённые имена для идентификаторов моделей:

```toml
[model_aliases]
grok3 = "grok-3-beta"
gpt4o = "gpt-4o"
ds3 = "deepseek-chat"
claude = "claude-sonnet-4-20250514"
```

Используйте алиасы с `-m`:

```bash
claudex run grok -m grok3
```

## Настройка профиля

```toml
[[profiles]]
name = "grok"
provider_type = "OpenAICompatible"
base_url = "https://api.x.ai/v1"
api_key = "xai-..."
# api_key_keyring = "grok-api-key"
default_model = "grok-3-beta"
auth_type = "api-key"              # "api-key" (по умолчанию) или "oauth"
# oauth_provider = "openai"        # обязателен при auth_type = "oauth"
backup_providers = ["deepseek"]
custom_headers = {}
extra_env = {}
priority = 100
enabled = true
max_tokens = 16384                  # опционально: ограничить выходные токены
strip_params = "auto"               # "auto" | "none" | ["temperature", "top_p"]

# URL query-параметры (например, Azure api-version)
[profiles.query_params]
# api-version = "2024-12-01-preview"

# Маппинг слотов моделей (опционально)
[profiles.models]
haiku = "grok-3-mini-beta"
sonnet = "grok-3-beta"
opus = "grok-3-beta"
```

| Поле | Тип | По умолчанию | Описание |
|------|-----|--------------|---------|
| `name` | string | *обязательно* | Уникальный идентификатор профиля |
| `provider_type` | string | `"DirectAnthropic"` | `"DirectAnthropic"`, `"OpenAICompatible"` или `"OpenAIResponses"` |
| `base_url` | string | *обязательно* | URL endpoint API провайдера |
| `api_key` | string | `""` | API-ключ в открытом тексте |
| `api_key_keyring` | string | — | Имя записи в связке ключей ОС (переопределяет `api_key`) |
| `default_model` | string | *обязательно* | Идентификатор модели по умолчанию |
| `auth_type` | string | `"api-key"` | Метод аутентификации: `"api-key"` или `"oauth"` |
| `oauth_provider` | string | — | Имя OAuth-провайдера. Один из: `claude`, `openai`, `google`, `qwen`, `kimi`, `github`, `gitlab` |
| `backup_providers` | string[] | `[]` | Имена профилей для failover, проверяются по порядку |
| `custom_headers` | map | `{}` | Дополнительные HTTP-заголовки, отправляемые с каждым запросом |
| `extra_env` | map | `{}` | Переменные окружения, устанавливаемые при запуске Claude |
| `priority` | integer | `100` | Вес приоритета для умной маршрутизации (выше = предпочтительнее) |
| `enabled` | boolean | `true` | Активен ли данный профиль |
| `max_tokens` | integer | — | Ограничить максимальное количество выходных токенов. При установке переопределяет `max_tokens` в запросах |
| `strip_params` | string/array | `"auto"` | Параметры для удаления из запросов. `"auto"` определяет известные endpoint-ы; `"none"` отправляет все; массив удаляет конкретные параметры |

### Query-параметры

```toml
[profiles.query_params]
api-version = "2024-12-01-preview"
```

| Поле | Тип | Описание |
|------|-----|---------|
| `query_params` | map | Пары ключ-значение, добавляемые как URL query-параметры к каждому запросу |

Используется в первую очередь для Azure OpenAI (`api-version`), но работает с любым провайдером.

### Маппинг слотов моделей

Опциональная таблица `[profiles.models]` сопоставляет слоты переключателя `/model` Claude Code с именами моделей, специфичными для провайдера. При переключении моделей внутри Claude Code (например, `/model opus`) Claudex переводит запрос на сопоставленную модель.

```toml
[profiles.models]
haiku = "grok-3-mini-beta"    # сопоставляет /model haiku
sonnet = "grok-3-beta"        # сопоставляет /model sonnet
opus = "grok-3-beta"          # сопоставляет /model opus
```

| Поле | Тип | Описание |
|------|-----|---------|
| `haiku` | string | Модель при выборе `haiku` в Claude Code |
| `sonnet` | string | Модель при выборе `sonnet` в Claude Code |
| `opus` | string | Модель при выборе `opus` в Claude Code |

<Aside type="tip">
  Если слот не определён, в качестве запасного значения используется `default_model` профиля. Слоты также экспортируются как переменные окружения: `ANTHROPIC_DEFAULT_HAIKU_MODEL`, `ANTHROPIC_DEFAULT_SONNET_MODEL`, `ANTHROPIC_DEFAULT_OPUS_MODEL`.
</Aside>

### Примеры провайдеров

```toml
# Anthropic (DirectAnthropic — без трансляции)
[[profiles]]
name = "anthropic"
provider_type = "DirectAnthropic"
base_url = "https://api.anthropic.com"
api_key = "sk-ant-..."
default_model = "claude-sonnet-4-20250514"

# MiniMax (DirectAnthropic — без трансляции)
[[profiles]]
name = "minimax"
provider_type = "DirectAnthropic"
base_url = "https://api.minimax.io/anthropic"
api_key = "..."
default_model = "claude-sonnet-4-20250514"
backup_providers = ["anthropic"]

# OpenRouter (OpenAICompatible — требует трансляции)
[[profiles]]
name = "openrouter"
provider_type = "OpenAICompatible"
base_url = "https://openrouter.ai/api/v1"
api_key = "..."
default_model = "anthropic/claude-sonnet-4"

# Grok (OpenAICompatible — требует трансляции)
[[profiles]]
name = "grok"
provider_type = "OpenAICompatible"
base_url = "https://api.x.ai/v1"
api_key = "xai-..."
default_model = "grok-3-beta"
backup_providers = ["deepseek"]

# OpenAI (OpenAICompatible — требует трансляции)
[[profiles]]
name = "chatgpt"
provider_type = "OpenAICompatible"
base_url = "https://api.openai.com/v1"
api_key = "sk-..."
default_model = "gpt-4o"

# DeepSeek (OpenAICompatible — требует трансляции)
[[profiles]]
name = "deepseek"
provider_type = "OpenAICompatible"
base_url = "https://api.deepseek.com"
api_key = "..."
default_model = "deepseek-chat"
backup_providers = ["grok"]

# Kimi / Moonshot (OpenAICompatible — требует трансляции)
[[profiles]]
name = "kimi"
provider_type = "OpenAICompatible"
base_url = "https://api.moonshot.ai/v1"
api_key = "..."
default_model = "kimi-k2-0905-preview"

# GLM / 智谱 (OpenAICompatible — требует трансляции)
[[profiles]]
name = "glm"
provider_type = "OpenAICompatible"
base_url = "https://api.z.ai/api/paas/v4"
api_key = "..."
default_model = "glm-4.6"

# Groq (OpenAICompatible — быстрый инференс)
[[profiles]]
name = "groq"
provider_type = "OpenAICompatible"
base_url = "https://api.groq.com/openai/v1"
api_key = "gsk_..."
default_model = "llama-3.3-70b-versatile"

# Mistral AI (OpenAICompatible — требует трансляции)
[[profiles]]
name = "mistral"
provider_type = "OpenAICompatible"
base_url = "https://api.mistral.ai/v1"
api_key = "..."
default_model = "mistral-large-latest"

# Together AI (OpenAICompatible — требует трансляции)
[[profiles]]
name = "together"
provider_type = "OpenAICompatible"
base_url = "https://api.together.xyz/v1"
api_key = "..."
default_model = "meta-llama/Llama-3.3-70B-Instruct-Turbo"

# Perplexity (OpenAICompatible — онлайн-поиск + LLM)
[[profiles]]
name = "perplexity"
provider_type = "OpenAICompatible"
base_url = "https://api.perplexity.ai"
api_key = "pplx-..."
default_model = "sonar-pro"

# Cerebras (OpenAICompatible — быстрый инференс)
[[profiles]]
name = "cerebras"
provider_type = "OpenAICompatible"
base_url = "https://api.cerebras.ai/v1"
api_key = "..."
default_model = "llama-3.3-70b"

# Azure OpenAI (OpenAICompatible — заголовок api-key + query_params)
[[profiles]]
name = "azure-openai"
provider_type = "OpenAICompatible"
base_url = "https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT"
api_key = "YOUR_AZURE_KEY"
default_model = "gpt-4o"
[profiles.query_params]
api-version = "2024-12-01-preview"

# Google Vertex AI (DirectAnthropic)
[[profiles]]
name = "vertex-ai"
provider_type = "DirectAnthropic"
base_url = "https://us-east5-aiplatform.googleapis.com/v1/projects/YOUR_PROJECT/locations/us-east5/publishers/anthropic/models"
api_key = "YOUR_GCLOUD_TOKEN"
default_model = "claude-sonnet-4@20250514"

# Ollama (локальный, API-ключ не нужен)
[[profiles]]
name = "local-qwen"
provider_type = "OpenAICompatible"
base_url = "http://localhost:11434/v1"
api_key = ""
default_model = "qwen2.5:72b"
enabled = false

# vLLM / LM Studio (локальный)
[[profiles]]
name = "local-llama"
provider_type = "OpenAICompatible"
base_url = "http://localhost:8000/v1"
api_key = ""
default_model = "llama-3.3-70b"
enabled = false
```

### Примеры OAuth-профилей

```toml
# Claude Max (пропускает прокси, использует нативный OAuth Claude из ~/.claude)
[[profiles]]
name = "claude-max"
provider_type = "DirectAnthropic"
base_url = "https://api.claude.ai"
default_model = "claude-sonnet-4-20250514"
auth_type = "oauth"
oauth_provider = "claude"

[profiles.models]
haiku = "claude-haiku-4-20250514"
sonnet = "claude-sonnet-4-20250514"
opus = "claude-opus-4-20250514"

# Подписка ChatGPT/Codex (OpenAIResponses)
[[profiles]]
name = "codex-sub"
provider_type = "OpenAIResponses"
base_url = "https://chatgpt.com/backend-api/codex"
default_model = "gpt-5.3-codex"
auth_type = "oauth"
oauth_provider = "openai"

[profiles.models]
haiku = "codex-mini-latest"
sonnet = "gpt-5.3-codex"
opus = "gpt-5.3-codex"

# Google Gemini через OAuth
[[profiles]]
name = "gemini-sub"
provider_type = "OpenAICompatible"
base_url = "https://generativelanguage.googleapis.com/v1beta/openai"
default_model = "gemini-2.5-pro"
auth_type = "oauth"
oauth_provider = "google"

# Kimi через OAuth
[[profiles]]
name = "kimi-oauth"
provider_type = "OpenAICompatible"
base_url = "https://api.moonshot.cn/v1"
default_model = "moonshot-v1-128k"
auth_type = "oauth"
oauth_provider = "kimi"

# Qwen через OAuth
[[profiles]]
name = "qwen-oauth"
provider_type = "OpenAICompatible"
base_url = "https://chat.qwen.ai/api"
default_model = "qwen3-235b-a22b"
auth_type = "oauth"
oauth_provider = "qwen"

# GitHub Copilot через OAuth
[[profiles]]
name = "copilot"
provider_type = "OpenAICompatible"
base_url = "https://api.githubcopilot.com"
default_model = "gpt-4o"
auth_type = "oauth"
oauth_provider = "github"

# GitLab Duo через GITLAB_TOKEN
[[profiles]]
name = "gitlab-duo"
provider_type = "OpenAICompatible"
base_url = "https://gitlab.com/api/v4/ai/llm/proxy"
default_model = "claude-sonnet-4-20250514"
auth_type = "oauth"
oauth_provider = "gitlab"
```

<Aside type="caution">
  Для OAuth-провайдера `claude` Claudex пропускает прокси и позволяет Claude Code использовать собственный OAuth напрямую. Для всех остальных OAuth-провайдеров прокси управляет токенами и их обновлением.
</Aside>

## Умный маршрутизатор

```toml
[router]
enabled = false
profile = "local-qwen"    # переиспользовать base_url + api_key из профиля
model = "qwen2.5:3b"      # переопределение модели (опционально)
```

| Поле | Тип | По умолчанию | Описание |
|------|-----|--------------|---------|
| `enabled` | boolean | `false` | Включить умную маршрутизацию |
| `profile` | string | `""` | Имя профиля для переиспользования при классификации (использует его `base_url` + `api_key`) |
| `model` | string | `""` | Переопределение модели для классификации (по умолчанию `default_model` профиля) |

### Правила маршрутизации

```toml
[router.rules]
code = "deepseek"
analysis = "grok"
creative = "chatgpt"
search = "kimi"
math = "deepseek"
default = "grok"
```

| Ключ | Описание |
|------|---------|
| `code` | Профиль для задач программирования |
| `analysis` | Профиль для анализа и логического рассуждения |
| `creative` | Профиль для творческого письма |
| `search` | Профиль для поиска и исследований |
| `math` | Профиль для математики и логики |
| `default` | Резерв при неклассифицированном намерении |

## Контекстный движок

### Сжатие

```toml
[context.compression]
enabled = false
threshold_tokens = 50000
keep_recent = 10
profile = "local-qwen"    # переиспользовать base_url + api_key из профиля
model = "qwen2.5:3b"      # переопределение модели (опционально)
```

| Поле | Тип | По умолчанию | Описание |
|------|-----|--------------|---------|
| `enabled` | boolean | `false` | Включить сжатие диалогов |
| `threshold_tokens` | integer | `50000` | Сжимать при превышении количества токенов |
| `keep_recent` | integer | `10` | Всегда хранить последние N сообщений несжатыми |
| `profile` | string | `""` | Имя профиля для переиспользования при суммаризации |
| `model` | string | `""` | Переопределение модели для суммаризации |

### Обмен контекстом между профилями

```toml
[context.sharing]
enabled = false
max_context_size = 2000
```

| Поле | Тип | По умолчанию | Описание |
|------|-----|--------------|---------|
| `enabled` | boolean | `false` | Включить обмен контекстом между профилями |
| `max_context_size` | integer | `2000` | Максимальное количество токенов для внедрения из других профилей |

### Локальный RAG

```toml
[context.rag]
enabled = false
index_paths = ["./src", "./docs"]
profile = "local-qwen"              # переиспользовать base_url + api_key из профиля
model = "nomic-embed-text"           # модель эмбеддингов
chunk_size = 512
top_k = 5
```

| Поле | Тип | По умолчанию | Описание |
|------|-----|--------------|---------|
| `enabled` | boolean | `false` | Включить локальный RAG |
| `index_paths` | string[] | `[]` | Каталоги для индексирования |
| `profile` | string | `""` | Имя профиля для переиспользования при создании эмбеддингов |
| `model` | string | `""` | Имя модели эмбеддингов |
| `chunk_size` | integer | `512` | Размер фрагмента текста в токенах |
| `top_k` | integer | `5` | Количество результатов для внедрения |
