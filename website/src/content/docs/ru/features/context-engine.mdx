---
title: Контекстный движок
description: Сжатие диалогов, совместное использование контекста между профилями и локальный RAG
---

import { Aside } from '@astrojs/starlight/components';

Контекстный движок улучшает взаимодействие с AI через три механизма: сжатие диалогов, совместное использование контекста между профилями и локальный RAG (Retrieval-Augmented Generation).

<Aside type="caution">
  Функции контекстного движка требуют службы LLM для создания резюме и генерации эмбеддингов. Можно использовать любой существующий профиль (локальный Ollama, облачные провайдеры типа OpenRouter и т.д.).
</Aside>

## Сжатие диалогов

Когда диалоги превышают пороговое значение токенов, Claudex использует LLM для создания резюме более старых сообщений, сохраняя последние нетронутыми.

```toml
[context.compression]
enabled = true
threshold_tokens = 50000    # сжимать при превышении этого порога токенов
keep_recent = 10            # всегда сохранять последние N сообщений
profile = "openrouter"      # переиспользовать base_url + api_key из профиля
model = "qwen/qwen-2.5-7b-instruct"  # переопределение модели (опционально)
```

### Как это работает

1. Перед пересылкой запроса Claudex оценивает общее количество токенов
2. Если токенов больше `threshold_tokens`, более старые сообщения (сверх `keep_recent`) заменяются резюме
3. Резюме создаётся настроенной локальной LLM
4. Сжатый диалог затем пересылается провайдеру

## Совместное использование контекста между профилями

Совместное использование контекста между разными профилями провайдеров в рамках одной сессии.

```toml
[context.sharing]
enabled = true
max_context_size = 2000    # максимальное количество токенов для внедрения из других профилей
```

Это полезно при переключении между провайдерами в середине задачи — релевантный контекст из предыдущих взаимодействий автоматически включается.

## Локальный RAG

Индексирование локального кода и документации для поисково-дополненной генерации. Релевантные фрагменты кода автоматически внедряются в запросы.

```toml
[context.rag]
enabled = true
index_paths = ["./src", "./docs"]     # директории для индексирования
profile = "openrouter"                 # переиспользовать base_url + api_key из профиля
model = "openai/text-embedding-3-small"  # модель эмбеддингов
chunk_size = 512                       # размер фрагмента текста
top_k = 5                             # количество результатов для внедрения
```

### Как это работает

1. При запуске Claudex индексирует файлы в `index_paths` с использованием модели эмбеддингов
2. Для каждого запроса сообщение пользователя преобразуется в эмбеддинг и сравнивается с индексом
3. Наиболее релевантные top-k фрагментов внедряются как дополнительный контекст в запрос
4. Провайдер получает более насыщенный контекст о вашей кодовой базе

<Aside type="tip">
  Для достижения наилучших результатов с локальным RAG используйте специализированную модель эмбеддингов, например `nomic-embed-text` через Ollama.
</Aside>
