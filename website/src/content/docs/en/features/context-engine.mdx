---
title: Context Engine
description: Conversation compression, cross-profile sharing, and local RAG
---

import { Aside } from '@astrojs/starlight/components';

The context engine enhances your AI interactions through three mechanisms: conversation compression, cross-profile context sharing, and local RAG (Retrieval-Augmented Generation).

<Aside type="caution">
  The context engine features require a local LLM service (like Ollama) for summarization and embedding generation.
</Aside>

## Conversation Compression

When conversations grow beyond a token threshold, Claudex uses a local LLM to summarize older messages, keeping recent ones intact.

```toml
[context.compression]
enabled = true
threshold_tokens = 50000    # compress when total tokens exceed this
keep_recent = 10            # always keep the last N messages
summarizer_url = "http://localhost:11434/v1"
summarizer_model = "qwen2.5:3b"
```

### How It Works

1. Before forwarding a request, Claudex estimates total token count
2. If tokens exceed `threshold_tokens`, older messages (beyond `keep_recent`) are replaced with a summary
3. The summary is generated by the configured local LLM
4. The compressed conversation is then forwarded to the provider

## Cross-Profile Sharing

Share context across different provider profiles within the same session.

```toml
[context.sharing]
enabled = true
max_context_size = 2000    # max tokens to inject from other profiles
```

This is useful when switching between providers mid-task â€” relevant context from previous interactions is automatically included.

## Local RAG

Index local code and documentation for retrieval-augmented generation. Relevant code snippets are automatically injected into requests.

```toml
[context.rag]
enabled = true
index_paths = ["./src", "./docs"]     # directories to index
embedding_url = "http://localhost:11434/v1"
embedding_model = "nomic-embed-text"  # embedding model
chunk_size = 512                       # text chunk size
top_k = 5                             # number of results to inject
```

### How It Works

1. On startup, Claudex indexes files in `index_paths` using the embedding model
2. For each request, the user's message is embedded and compared against the index
3. The top-k most relevant chunks are injected as additional context in the request
4. The provider receives richer context about your codebase

<Aside type="tip">
  For best results with local RAG, use a dedicated embedding model like `nomic-embed-text` via Ollama.
</Aside>
